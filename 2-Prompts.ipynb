{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ddc08b",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88bdc813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate, load_prompt, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03244660",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5130608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=700\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572d1a2",
   "metadata": {},
   "source": [
    "### Static & Dynamic Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb082df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word2vec paper introduces two architectures: Continuous Bag-of-Words (CBOW) and Continuous Skip-Gram. CBOW predicts a word based on its context, while Skip-Gram predicts the context based on a word. Both models learn vector representations of words, capturing semantic relationships. The vectors are learned by optimizing the prediction task using stochastic gradient descent. The resulting word embeddings exhibit linear structure, enabling analogical reasoning.\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(\"Summarize word2vec paper in 5 lines.\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a392b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"\"\"Please summarize the research paper titled {paper_input} with the following specifications:\n",
    "        Explanation Style: {style_input}  \n",
    "        Explanation Length: {length_input}  \n",
    "        1. Mathematical Details: \n",
    "           - Include relevant mathematical equations if present in the paper.  \n",
    "           - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
    "        2. Analogies:\n",
    "           - Use relatable analogies to simplify complex ideas.\n",
    "        If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
    "        Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
    "    \"\"\",\n",
    "    input_variables=[\"paper_input\",\"style_input\",\"length_input\"],\n",
    "    validate_template=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae997318",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.invoke({\n",
    "    'paper_input': \"word2vec: Efficient Estimation of Word Representations in Vector Space\",\n",
    "    'style_input': \"Technical\",\n",
    "    'length_input': \"5 lines\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143f68b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"word2vec: Efficient Estimation of Word Representations in Vector Space\" proposes two architectures for learning word representations: Continuous Bag-of-Words (CBOW) and Skip-Gram.\n",
      "\n",
      "**Key Points (5 lines):**\n",
      "The CBOW architecture predicts a target word based on its context words, while Skip-Gram predicts the context words based on the target word. \n",
      "The objective function for Skip-Gram is: `L = Σ log(p(w_O|w_I))` where `w_O` is the output word and `w_I` is the input word. \n",
      "This is optimized using Negative Sampling: `p(w_O|w_I) = σ(v_{w_O}^T v_{w_I}) \\* ∏[1 - σ(v_{w_N}^T v_{w_I})]` where `w_N` are negative samples. \n",
      "Mathematically, this can be represented as a dot product of word vectors: `v_{w_O}^T v_{w_I}`. \n",
      "The learned word representations capture semantic relationships, e.g., `v(\"king\") - v(\"man\") + v(\"woman\") ≈ v(\"queen\")`, illustrating the vector space's ability to encode analogies.\n",
      "\n",
      "**Analogy:** Think of word vectors as points on a map, where semantically similar words are closer together, just like cities with similar climates are geographically proximal.\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe8384",
   "metadata": {},
   "source": [
    "### Prompt Reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26159e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template.save(\"template.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eff90e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = load_prompt(\"template.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06154af",
   "metadata": {},
   "source": [
    "### Template and Model Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3321c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb0115bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a beginner-friendly summary of the \"word2vec: Efficient Estimation of Word Representations in Vector Space\" paper in 5 lines:\n",
      "\n",
      "The word2vec paper introduces a method to represent words as vectors in a high-dimensional space, capturing their semantic meanings. It uses two architectures: Continuous Bag of Words (CBOW) and Skip-Gram. The Skip-Gram model predicts surrounding words given a center word, maximizing the objective function: `1/T * Σ log p(w_t+j | w_t)`, where `p(w_t+j | w_t) = exp(v_w_t * v_w_t+j) / Σ exp(v_w_t * v_w')`. Think of it like a word predictor game, where the model learns to identify likely neighboring words. The resulting vector representations can be used for tasks like text classification and clustering.\n",
      "\n",
      "Mathematical Concept: The probability `p(w_t+j | w_t)` is calculated using the dot product of word vectors (`v_w_t * v_w_t+j`), representing the similarity between words.\n",
      "\n",
      "Analogy: Imagine a map where similar words are placed near each other, like cities with similar climates. The word2vec model creates such a map, allowing us to navigate and understand word relationships.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\n",
    "    'paper_input': \"word2vec: Efficient Estimation of Word Representations in Vector Space\",\n",
    "    'style_input': \"Beginner-friendly\",\n",
    "    'length_input': \"5 lines\"\n",
    "})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d7c18",
   "metadata": {},
   "source": [
    "### Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb02f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    SystemMessage(content='You are a helpful AI assistant')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f947600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='exit', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input('You: ')\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    result = model.invoke(chat_history)\n",
    "    chat_history.append(AIMessage(content=result.content))\n",
    "    print(\"AI: \",result.content)\n",
    "print(chat_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d45f88",
   "metadata": {},
   "source": [
    "### Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae4dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate([\n",
    "    ('system',\"You are a helpful {domain} expert\"),\n",
    "    ('human',\"Explain in simple terms, what is {topic}\")\n",
    "    # SystemMessage(content=\"You are a helpful {domain} expert\"),\n",
    "    # HumanMessage(content=\"Explain in simple terms, what is {topic}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d01086",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({\n",
    "    'domain': 'AI',\n",
    "    'topic': 'tokenization'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd329291",
   "metadata": {},
   "source": [
    "### Message Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb3b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate([\n",
    "    ('system','You are a helpful customer support agent'),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    ('human','{query}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc208c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "with open('chat_history.txt') as f:\n",
    "    chat_history.extend(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e98bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'Where is my refund'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
